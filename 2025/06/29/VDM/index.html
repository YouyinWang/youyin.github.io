<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Fira Code:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-minimal.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"example.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"flat"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":"gitalk","storage":true,"lazyload":false,"nav":null,"activeClass":"gitalk"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="references:   Understanding Diffusion Models: A Unified Perspective https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2208.11970  Denoising Diffusion Probabilistic Models https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.11239    近年来，Diffusion Model在">
<meta property="og:type" content="article">
<meta property="og:title" content="Diffusion Model的理论基础">
<meta property="og:url" content="http://example.com/2025/06/29/VDM/index.html">
<meta property="og:site_name" content="输出倒逼输入">
<meta property="og:description" content="references:   Understanding Diffusion Models: A Unified Perspective https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2208.11970  Denoising Diffusion Probabilistic Models https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2006.11239    近年来，Diffusion Model在">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="c:\Users\youyinwang\AppData\Roaming\Typora\typora-user-images\image-20250623225923651.png">
<meta property="og:image" content="c:\Users\youyinwang\Desktop\Notes\Difussion\VDM-process.png">
<meta property="article:published_time" content="2025-06-29T13:00:00.000Z">
<meta property="article:modified_time" content="2025-06-29T14:07:27.378Z">
<meta property="article:author" content="Youyin Wang">
<meta property="article:tag" content="Diffusion">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="c:\Users\youyinwang\AppData\Roaming\Typora\typora-user-images\image-20250623225923651.png">

<link rel="canonical" href="http://example.com/2025/06/29/VDM/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>
<!-- MathJax Configuration -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ['\\(','\\)'] ],
    displayMath: [ ['$$','$$'], ['\\[','\\]'] ],
    processEscapes: true
  },
  TeX: {
    equationNumbers: { autoNumber: "AMS" },
    extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  },
  "HTML-CSS": {
    availableFonts: ["STIX", "TeX"],
    linebreaks: { automatic: true },
    imageFont: null
  },
  SVG: {
    font: "STIX-Web",
    linebreaks: { automatic: true },
    imageFont: null
  }
});
</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> 
  <title>Diffusion Model的理论基础 | 输出倒逼输入</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>
    <a target="_blank" rel="noopener" href="https://github.com/YouyinWang" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">输出倒逼输入</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/06/29/VDM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.png">
      <meta itemprop="name" content="Youyin Wang">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="输出倒逼输入">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Diffusion Model的理论基础
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2025-06-29 21:00:00 / 修改时间：22:07:27" itemprop="dateCreated datePublished" datetime="2025-06-29T21:00:00+08:00">2025-06-29</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <blockquote>
<p>references: </p>
<ul>
<li><p>Understanding Diffusion Models: A Unified Perspective <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2208.11970">https://arxiv.org/abs/2208.11970</a></p>
</li>
<li><p>Denoising Diffusion Probabilistic Models <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a></p>
</li>
</ul>
</blockquote>
<p>近年来，Diffusion Model在图像/视频生成领域以及VLA模型都表现出优异性能，本文参考上述文献，试图以简单扼要的篇幅解释清楚Diffusion Model的数学原理，适合有CS和Machine Learning背景的读者快速入门。</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>生成模型领域有几个well-known的方向，包括 Generative Adversarial Networks (GANs)，likelihood-based，energy-based以及score-based，Variational Diffusion Models (VDM) 的数学原理可以从多个方向进行解释，本文选择likelihood-based，个人认为比较好理解。</p>
<p>likelihood-based的方法的核心思想是：给定一个数据集 $X<em>D$，训练模型来最大化数据集中样本（evidence）的出现概率（likelihood），进而拟合到$P</em>{\phi}(x)$，即数据的真实分布。这类方法中最具代表性的就是Variational Autoencoders (VAEs)，而VDM可以认为VAE的一个变体，因此本文从VAE讲起，逐步递进至VDM。</p>
<h2 id="VAE"><a href="#VAE" class="headerlink" title="VAE"></a>VAE</h2><p>likelihood-based生成模型的核心目标是拟合出数据的真实分布（$P_{\phi}(x) \rightarrow P(x)$)，但只有数据分布还不够，因为数据生成过程本质是在真实的数据分布中采样，对于一个完全未知的复杂分布是很难采样的，因此，VAE借助一个随机变量<script type="math/tex">z</script>和自定义分布$P(z)$（一般选择多维的标准高斯分布$N~(z; 0,I)$ ），并通过$q(z|x)$（Encoder）和$p(x|z)$（Decoder）将$z$和$x$建立起联系。这样，对符合高斯分布的$z$采样，就能通过Decoder得到一个新的生成数据$x’$，$z$被叫做隐变量（latent variable）。</p>
<blockquote>
<p>所谓Variational Autoencoders，Autoencoder是指通过编解码的过程，将数据压缩至低维空间；Variational是指对隐空间（latent space）施加约束，迫使它成为正态分布，进而为整个数据空间提供了生成能力。</p>
</blockquote>
<p>然而直接最大化likelihood是困难的，最大化某个复杂变量常用的方法是最大化它的某个下界。由引理1可以把likelihood <script type="math/tex">logP(x)</script> 拆成两项，即$E<em>{q</em>{\phi}(z|x)} [log \frac {p(x,z)} {q<em>{\phi}(z|x)}]$和$D</em>{KL}[q_{\phi}(z|x)||p(z|x)]$，KL散度的定义决定了它是大于等于0的，所以第一项就是likelihood的一个下界，称做<strong>Evidence Lower Bound (ELBO)</strong>。</p>
<p>进一步分析最大化ELBO是在做什么，由lemma 1得 <script type="math/tex">logP(x) = E_{q_{\phi}(z|x)} [log \frac {p(x,z)} {q_{\phi}(z|x)}] + D_{KL}[q_{\phi}(z|x)||p(z|x)]</script>，等号左边和参数$\Phi$没关系（可以视作关于$\Phi$ 定值），因此通过调整参数$\Phi$来最大化ELBO，实际上就是在最小化$q_{\phi}(z|x)$Encoder和真实的后验概率$p(z|x)$之间的KL散度，这正是我们想要的。完美！</p>
<p>这里你可能想问，最大化ELBO就是在优化encoder了，那decoder在哪里优化呢？答案就是在ELBO里面了，接下来继续分析ELBO。</p>
<p>由lemma 2可知，$ELBO = E<em>{q</em>{\phi}(z|x)} [log {p<em>{\theta}(x|z)}] - D</em>{KL}(q_{\phi}(z|x)||p(z))$，最大化ELBO就是要最大化第一项并最小化第二项。拆开来看，第一项是在Decoder从隐变量z生成原始数据x的概率，被称为reconstruction term，用于优化Decoder；第二项是由Encoder映射到的隐变量z分布和指定的分布$p(z)$之间的KL散度，被称作prior matching term，用于优化Encoder。</p>
<p>这里结合一下VAE模型的实际实现，进一步理解上述的两个优化项在工程中是如何实现的。在实际的VAE的模型中，Encoder拟合的变分分布通常被假设为一个对角高斯（各维度独立的正态分布），即$q<em>{\phi}(z|x) = N(z;\mu</em>{\phi}(x),\delta^2_{\phi}(x))$，而prior分布$p(z)$通常选取标准的多元高斯$N(z; 0, I)$。对于prior matching term，KL散度是可以直接计算的；对于reconstruction term，期望的计算则往往利用蒙特卡洛估计进行近似，即</p>
<script type="math/tex; mode=display">\arg\max_{\phi, \theta} \mathbb{E}_{q_{\phi}(z|x)} \left[ \log p_{\theta}(x|z) \right] - D_{KL}(q_{\phi}(z|x) \parallel p(z)) \approx \arg\max_{\phi, \theta} \sum_{l=1}^{L} \log p_{\theta}(x|z^{(l)}) - D_{KL}(q_{\phi}(z|x) \parallel p(z))</script><p>而直接采样的操作是不可导的，需要用重参数化技巧，非本文重点不多展开。</p>
<h2 id="MHVAE-（Markovian-Hierarchical-Variational-Autoencoders）"><a href="#MHVAE-（Markovian-Hierarchical-Variational-Autoencoders）" class="headerlink" title="MHVAE （Markovian Hierarchical Variational Autoencoders）"></a>MHVAE （Markovian Hierarchical Variational Autoencoders）</h2><p>MHVAE是一种特殊的VAE，它引入了T个隐变量$z_{1:T}$，将编/解码的过程分成T步，也就是所谓的级联Hierarchical。同时，每一步的编/解码只和它的上一步状态有关，和更早的状态无关，即马尔可夫性质Markovian。</p>
<p><img src="C:\Users\youyinwang\AppData\Roaming\Typora\typora-user-images\image-20250623225923651.png" alt="image-20250623225923651" style="zoom:40%;" /></p>
<p>由lemma 1易得，<script type="math/tex">logp(x) \ge \mathbb{E}_{q_{\phi}(z_{1:T}|x)} \left[ \log \frac{p(x, z_{1:T})}{q_{\phi}(z_{1:T}|x)} \right]</script>。</p>
<h2 id="VDM-Variational-Diffusion-Model"><a href="#VDM-Variational-Diffusion-Model" class="headerlink" title="VDM (Variational Diffusion Model)"></a>VDM (Variational Diffusion Model)</h2><p>VDM是MHVAE在以下三个限制条件的特例：</p>
<ol>
<li>数据$x$和所有隐变量$z_t$的<strong>维度相同</strong>；</li>
<li><strong>所有的encoder $q(z<em>t|z</em>{t-1})$都是预定义好的高斯分布模型</strong>（不需要学习），即$z<em>t$状态为以$z</em>{t-1}$为均值的高斯分布；</li>
<li>最终T状态的分布$z_T$为<strong>标准高斯分布</strong>。</li>
</ol>
<p>这里做一下符号变换 $x\rightarrow x<em>0, z_t-&gt;x_t$，限制1和2指定了$q(x_t|x</em>{t-1}) \sim N(x<em>t, \sqrt\alpha_tx</em>{t-1}, (1-\alpha_t)I)$, $\alpha_t$是VDM中的超参，一般认为指定，有些后续方法也将其设定为可学习。限制3指定$p(x_T) \sim N(0,I)$。</p>
<p> 既然VDM是VAE的变体，那思路不变，最大化Evidence的过程依然是去最大化ELBO，接下来就讲一下如何获得VDM的ELBO。</p>
<p>由lemma 3可得，<script type="math/tex">ELBO = \underbrace{\mathbb{E}_{q(x_1|x_0)}[\log p_{\theta}(x_0|x_1)]}_{\text{reconstruction term}} - \underbrace{\mathbb{E}_{q(x_{T-1}|x_0)}[D_{KL}(q(x_T|x_{T-1}) \parallel p(x_T))]}_{\text{prior matching term}} - \sum_{t=1}^{T-1} \underbrace{\mathbb{E}_{q(x_{t-1}, x_{t+1}|x_0)}[D_{KL}(q(x_t|x_{t-1}) \parallel p_{\theta}(x_t|x_{t+1}))]}_{\text{consistency term}}</script>，接下来看一下这三项的物理含义。</p>
<ol>
<li>第一项被称作reconstruction term，表示的是基于第一步的隐变量原始数据分布的log probability；</li>
<li>第二项被称作prior matching term，表示最后一步的encode分布和最终隐变量的分布之间的KL散度，其中没有可优化的参数，由于我们假设在T足够大时$p(x_T)$也是高斯，这一项可以视作0；</li>
<li>第三项被称作consistency term，表示对于某个中间隐状态，denoising step from a noisier image should match the corresponding noising step from a cleaner image。</li>
</ol>
<p><img src="C:\Users\youyinwang\Desktop\Notes\Difussion\VDM-process.png" alt="image-20250627085722712" style="zoom:30%;" /></p>
<p>目前ELBO中的三项都是期望，需要通过蒙特卡洛方法进行估计，然而对于consistency term，它依据的两个随机变量（$x<em>{t-1}$和$x</em>{t+1}$）的期望，这会使得方差变大，估计值更加不准，因此需要进一步推导。</p>
<p>利用马尔可夫性质和贝叶斯公式，可得$q(x<em>t|x</em>{t-1}) = q(x<em>t|x</em>{t-1}, x<em>0) = \frac{q(x</em>{t-1}|x<em>t, x_0)q(x_t|x_0)}{q(x</em>{t-1}|x<em>0)}$，将这个关键步骤带入ELBO得推导过程（具体过程见lemma 4），可得$$ELBO = \underbrace{\mathbb{E}</em>{q(x<em>1|x_0)}[\log p</em>{\theta}(x<em>0|x_1)]}</em>{\text{reconstruction error}} - \underbrace{D<em>{KL}(q(x_T|x_0) \parallel p(x_T))}</em>{\text{prior matching term}} - \sum<em>{t=2}^{T} \underbrace{\mathbb{E}</em>{q(x<em>t|x_0)}[D</em>{KL}(q(x<em>{t-1}|x_t, x_0) \parallel p</em>{\theta}(x<em>{t-1}|x_t))]}</em>{\text{transition matching terms}}$$ 。第一项和VAE类似不再赘述，第二项没有可优化参数可以忽略，重点关注第三项，$q(x_{t-1}|x_t,x_0)$是未知分布，直接拟合是比较困难的，需要对其进行进一步推导。</p>
<p>由于$q(x<em>t|x</em>{t-1})$是预定义好的高斯分布，容易想到用贝叶斯公式对$q(x<em>{t-1}|x_t, x_0)$进项转化，$q(x</em>{t-1}|x<em>t,x_0) = \frac{q(x_t|x</em>{t-1},x<em>0)q(x</em>{t-1}|x<em>0)}{q(x_t|x_0)}$，但其中还带了 $q(x</em>{t-1}|x<em>0)$和$q(x_t|x_0)$，由lemma 5得，它们也是高斯分布，$x_t \sim N(x_t; \sqrt{\overline\alpha_t} x_0, (1-\overline\alpha_t)I$，其中$\overline\alpha_t=\prod</em>{i=0}^t{\alpha<em>t}$。带入推导过程，可以得到$q(x</em>{t-1}|x<em>t,x_0)$也是一个高斯分布（具体过程见lemma 6），$x</em>{t-1} \sim N(x<em>{t-1}; \frac{\sqrt{\alpha_t}(1-\bar{\alpha}</em>{t-1})x<em>t + \sqrt{\bar{\alpha}</em>{t-1}}(1-\alpha<em>t)x_0}{1-\bar{\alpha}_t}, \frac{(1-\alpha_t)(1-\bar{\alpha}</em>{t-1})}{1-\bar{\alpha}<em>t}\mathbf{I})$ 。我们假设$p</em>\theta(x<em>{t-1}|x_t)$也是高斯分布，由于$x_t$以及$\alpha_t,\bar{\alpha_t}$在这个分布中都是给定的，为了方便对应，可以将$p</em>\theta(x<em>{t-1}|x_t)$的形式设置为$N(x</em>{t-1}; \frac{\sqrt{\alpha<em>t}(1-\bar{\alpha}</em>{t-1})x<em>t + \sqrt{\bar{\alpha}</em>{t-1}}(1-\alpha<em>t)\hat{x</em>{\theta}}(x<em>t,t)}{1-\bar{\alpha}_t}, \frac{(1-\alpha_t)(1-\bar{\alpha}</em>{t-1})}{1-\bar{\alpha}<em>t}\mathbf{I})$，那么两个高斯分布直接之间KL散度$D</em>{KL}(q(x<em>{t-1}|x_t, x_0) \parallel p</em>{\theta}(x<em>{t-1}|x_t))$就可以转化为$x</em>\theta(x_t,t)$和$x_0$之间的差距了，由lemma 7可得，</p>
<script type="math/tex; mode=display">D_{KL}(q(x_{t-1}|x_t, x_0) \parallel p_{\theta}(x_{t-1}|x_t)) = \frac{1}{2} \left( \frac{\bar{\alpha}_{t-1}}{1 - \bar{\alpha}_{t-1}} - \frac{\bar{\alpha}_t}{1 - \bar{\alpha}_t} \right) \left[ \| \hat{x}_{\theta}(x_t, t) - x_0 \|_2^2 \right]</script><p>到目前为止，优化目标已经被转为训练一个映射$\hat x_\theta$，它基于步数t和第t步得隐变量$x_t$，恢复出$x_0$，这已经是一个可训练的目标了。另外，还可以利用$x_0 = \frac{x_t-\sqrt{1-\overline\alpha_t}\epsilon_0}{\sqrt{\overline\alpha_t}}$，将优化目标转化为预测第t步加在$x_0$上的噪声（具体过程见lemma 8）。实践结论表明，预测噪声效果会更好，DDPM就是使用这种方式。</p>
<h2 id="Lemmas"><a href="#Lemmas" class="headerlink" title="Lemmas"></a>Lemmas</h2><h3 id="lemma1-拆分likelihood获得ELBO"><a href="#lemma1-拆分likelihood获得ELBO" class="headerlink" title="lemma1: 拆分likelihood获得ELBO"></a>lemma1: 拆分likelihood获得ELBO</h3><script type="math/tex; mode=display">
\begin{align*}
\log p(x) &= \log p(x) \int q_\phi(z|x) dz \\
&= \int q_\phi(z|x) (\log p(x)) dz \\
&= \mathbb{E}_{q_\phi(z|x)} [\log p(x)] \\
&= \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p(x, z)}{p(z|x)} \right] \\
&= \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p(x, z) q_\phi(z|x)}{p(z|x) q_\phi(z|x)} \right] \\
&= \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p(x, z)}{q_\phi(z|x)} \right] + \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{q_\phi(z|x)}{p(z|x)} \right] \\
&= \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p(x, z)}{q_\phi(z|x)} \right] + D_{KL}(q_\phi(z|x) \parallel p(z|x)) \\
&\geq \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p(x, z)}{q_\phi(z|x)} \right]
\end{align*}</script><h3 id="lemma2-ELBO分解"><a href="#lemma2-ELBO分解" class="headerlink" title="lemma2: ELBO分解"></a>lemma2: ELBO分解</h3><script type="math/tex; mode=display">
\begin{align*}
\mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p(x, z)}{q_\phi(z|x)} \right] &= \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p_\theta(x|z) p(z)}{q_\phi(z|x)} \right] \\
&= \mathbb{E}_{q_\phi(z|x)} [\log p_\theta(x|z)] + \mathbb{E}_{q_\phi(z|x)} \left[ \log \frac{p(z)}{q_\phi(z|x)} \right] \\
&= \underbrace{\mathbb{E}_{q_\phi(z|x)} [\log p_\theta(x|z)]}_{\text{reconstruction term}} - \underbrace{D_{KL}(q_\phi(z|x) \parallel p(z))}_{\text{prior matching term}}
\end{align*}</script><h3 id="lemma3-VDM-ELBO推导1"><a href="#lemma3-VDM-ELBO推导1" class="headerlink" title="lemma3: VDM ELBO推导1"></a>lemma3: VDM ELBO推导1</h3><script type="math/tex; mode=display">
\begin{align*}
\log p(x) &= \log \int p(x_{0:T}) q(x_{1:T}|x_0) dx_{1:T} \\
&= \log \int \frac{p(x_{0:T}) q(x_{1:T}|x_0)}{q(x_{1:T}|x_0)} dx_{1:T} \\
&= \log \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \frac{p(x_{0:T})}{q(x_{1:T}|x_0)} \right] \\
&\geq \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{p(x_{0:T})}{q(x_{1:T}|x_0)} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{p(x_T) \prod_{t=1}^T p_\theta(x_{t-1}|x_t)}{\prod_{t=1}^T q(x_t|x_{t-1})} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{p(x_T) p_\theta(x_0|x_1) \prod_{t=2}^T p_\theta(x_{t-1}|x_t)}{q(x_T|x_{T-1}) \prod_{t=1}^{T-1} q(x_t|x_{t-1})} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{p(x_T) p_\theta(x_0|x_1) \prod_{t=1}^{T-1} p_\theta(x_t|x_{t+1})}{q(x_T|x_{T-1}) \prod_{t=1}^{T-1} q(x_t|x_{t-1})} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{p(x_T) p_\theta(x_0|x_1)}{q(x_T|x_{T-1})} \right] + \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \prod_{t=1}^{T-1} \frac{p_\theta(x_t|x_{t+1})}{q(x_t|x_{t-1})} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} [\log p_\theta(x_0|x_1)] + \underbrace{\mathbb{E}_{q(x_{T-1}, x_T|x_0)} \left[ \log \frac{p(x_T)}{q(x_T|x_{T-1})} \right]}_{\text{prior matching term}} + \sum_{t=1}^{T-1} \mathbb{E}_{q(x_{1:t}, x_{t+1}|x_0)} \left[ \log \frac{p_\theta(x_t|x_{t+1})}{q(x_t|x_{t-1})} \right] \\
&= \underbrace{\mathbb{E}_{q(x_1|x_0)} [\log p_\theta(x_0|x_1)]}_{\text{reconstruction term}} + \underbrace{\mathbb{E}_{q(x_{T-1}, x_T|x_0)} \left[ D_{KL}(q(x_T|x_{T-1}) \parallel p(x_T)) \right]}_{\text{prior matching term}} \\
&\quad - \sum_{t=1}^{T-1} \underbrace{\mathbb{E}_{q(x_{t-1}, x_{t+1}|x_0)} \left[ D_{KL}(q(x_t|x_{t-1}) \parallel p_\theta(x_t|x_{t+1})) \right]}_{\text{consistency term}}
\end{align*}</script><ol>
<li>VDM ELBO推导2</li>
</ol>
<script type="math/tex; mode=display">
\begin{align*}
\log p(x) &\geq \mathbb{E}_{q(x_{1:T} | x_0)} \left[ \log \frac{p(x_{0:T})}{q(x_{1:T} | x_0)} \right] \\
&= \mathbb{E}_{q(x_{1:T} | x_0)} \left[ \log \frac{p(x_T) \prod_{t=1}^T p_\theta(x_{t-1} | x_t)}{\prod_{t=1}^T q(x_t | x_{t-1})} \right] \\
&= \mathbb{E}_{q(x_{1:T} | x_0)} \left[ \log \frac{p(x_T) p_\theta(x_0 | x_1) \prod_{t=2}^T p_\theta(x_{t-1} | x_t)}{q(x_1 | x_0) \prod_{t=2}^T q(x_t | x_{t-1})} \right] \\
&= \mathbb{E}_{q(x_{1:T} | x_0)} \left[ \log \frac{p(x_T) p_\theta(x_0 | x_1) \prod_{t=2}^T p_\theta(x_{t-1} | x_t)}{q(x_1 | x_0) \prod_{t=2}^T q(x_t | x_{t-1}, x_0)} \right] \\
&= \mathbb{E}_{q(x_{1:T} | x_0)} \left[ \log \frac{p_\theta(x_T) p_\theta(x_0 | x_1)}{q(x_1 | x_0)} + \log \prod_{t=2}^T \frac{p_\theta(x_{t-1} | x_t)}{q(x_t | x_{t-1}, x_0)} \right] \\
&= \mathbb{E}_{q(x_{1:T} | x_0)} \left[ \log \frac{p(x_T) p_\theta(x_0 | x_1)}{q(x_1 | x_0)} + \log \prod_{t=2}^T \frac{p_\theta(x_{t-1} | x_t)}{\frac{q(x_{t-1} | x_t, x_0) q(x_t | x_0)}{q(x_{t-1} | x_0)}} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{p(x_T) p_\theta(x_0|x_1)}{q(x_1|x_0)} + \log \prod_{t=2}^T \frac{p_\theta(x_{t-1}|x_t)}{\frac{q(x_{t-1}|x_t, x_0) q(x_t|x_0)}{q(x_{t-1}|x_0)}} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{p(x_T) p_\theta(x_0|x_1)}{q(x_1|x_0)} + \log \frac{q(x_1|x_0)}{q(x_T|x_0)} + \log \prod_{t=2}^T \frac{p_\theta(x_{t-1}|x_t)}{q(x_{t-1}|x_t, x_0)} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log \frac{p(x_T) p_\theta(x_0|x_1)}{q(x_T|x_0)} + \sum_{t=2}^T \log \frac{p_\theta(x_{t-1}|x_t)}{q(x_{t-1}|x_t, x_0)} \right] \\
&= \mathbb{E}_{q(x_{1:T}|x_0)} \left[ \log p_\theta(x_0|x_1) \right] + \mathbb{E}_{q(x_T|x_0)} \left[ \log \frac{p(x_T)}{q(x_T|x_0)} \right] + \sum_{t=2}^T \mathbb{E}_{q(x_{t}, x_{t-1}|x_0)} \left[ \log \frac{p_\theta(x_{t-1}|x_t)}{q(x_{t-1}|x_t, x_0)} \right] \\
&= \underbrace{\mathbb{E}_{q(x_1|x_0)} \left[ \log p_\theta(x_0|x_1) \right]}_{\text{reconstruction term}} - \underbrace{D_{KL}(q(x_T|x_0) \parallel p(x_T))}_{\text{prior matching term}} - \sum_{t=2}^T \underbrace{\mathbb{E}_{q(x_t|x_0)} \left[ D_{KL}(q(x_{t-1}|x_t, x_0) \parallel p_\theta(x_{t-1}|x_t)) \right]}_{\text{denoising matching term}}
\end{align*}</script><h3 id="lemma-5-p-x-t-x-0-得推导过程"><a href="#lemma-5-p-x-t-x-0-得推导过程" class="headerlink" title="lemma 5: $p(x_t|x_0)$得推导过程"></a>lemma 5: $p(x_t|x_0)$得推导过程</h3><script type="math/tex; mode=display">
\begin{align*}
x_t &= \sqrt{\alpha_t} x_{t-1} + \sqrt{1 - \alpha_t} \epsilon^*_{t-1} \\
&= \sqrt{\alpha_t} \left( \sqrt{\alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_{t-1}} \epsilon^*_{t-2} \right) + \sqrt{1 - \alpha_t} \epsilon^*_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{\alpha_t - \alpha_t \alpha_{t-1}} \epsilon^*_{t-2} + \sqrt{1 - \alpha_t} \epsilon^*_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{\alpha_t - \alpha_t \alpha_{t-1}} \epsilon^*_{t-2} + \sqrt{1 - \alpha_t} \epsilon^*_{t-1} \\
&= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{\alpha_t - \alpha_t \alpha_{t-1} + 1 - \alpha_t} \epsilon_{t-2} \\
&= \sqrt{\alpha_t \alpha_{t-1}} x_{t-2} + \sqrt{1 - \alpha_t \alpha_{t-1}} \epsilon_{t-2} \\
&= \ldots \\
&= \sqrt{\prod_{i=1}^{t} \alpha_i} x_0 + \sqrt{1 - \prod_{i=1}^{t} \alpha_i} \epsilon_0 \\
&= \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1 - \bar{\alpha}_t} \epsilon_0 \\
&\sim \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t} x_0, (1 - \bar{\alpha}_t) \mathbf{I})
\end{align*}</script><h3 id="lemma-6-p-x-t-1-x-t-x-0-推导过程"><a href="#lemma-6-p-x-t-1-x-t-x-0-推导过程" class="headerlink" title="lemma 6: $p(x{t-1}|x{t},x_0)$推导过程"></a>lemma 6: $p(x<em>{t-1}|x</em>{t},x_0)$推导过程</h3><script type="math/tex; mode=display">
\begin{align*}
q(x_{t-1}|x_t, x_0) &= \frac{q(x_t|x_{t-1}, x_0) q(x_{t-1}|x_0)}{q(x_t|x_0)} \\
&= \frac{\mathcal{N}(x_t; \sqrt{\alpha_t} x_{t-1}, (1 - \alpha_t) \mathbf{I}) \mathcal{N}(x_{t-1}; \sqrt{\alpha_{t-1}} x_0, (1 - \alpha_{t-1}) \mathbf{I})}{\mathcal{N}(x_t; \sqrt{\alpha_t} x_0, (1 - \alpha_t) \mathbf{I})} \\
&\propto \exp \left\{ -\frac{1}{2(1 - \alpha_t)} \left[ (x_t - \sqrt{\alpha_t} x_{t-1})^2 + (x_{t-1} - \sqrt{\alpha_{t-1}} x_0)^2 - (x_t - \sqrt{\alpha_t} x_0)^2 \right] \right\} \\
&= \exp \left\{ -\frac{1}{2} \left[ \frac{(x_t - \sqrt{\alpha_t} x_{t-1})^2}{1 - \alpha_t} + \frac{(x_{t-1} - \sqrt{\alpha_{t-1}} x_0)^2}{1 - \alpha_{t-1}} - \frac{(x_t - \sqrt{\alpha_t} x_0)^2}{1 - \alpha_t} \right] \right\} \\
&= \exp \left\{ -\frac{1}{2} \left[ \frac{2\sqrt{\alpha_t} x_t x_{t-1} + \alpha_t x_{t-1}^2}{1 - \alpha_t} + \frac{(x_{t-1}^2 - 2\sqrt{\alpha_{t-1}} x_{t-1} x_0)}{1 - \alpha_{t-1}} \right] + C(x_t, x_0) \right\} \\
&\propto \exp \left\{ -\frac{1}{2} \left[ \frac{2\sqrt{\alpha_t} x_t x_{t-1} + \alpha_t x_{t-1}^2}{1 - \alpha_t} + \frac{x_{t-1}^2}{1 - \alpha_{t-1}} - \frac{2\sqrt{\alpha_{t-1}} x_{t-1} x_0}{1 - \alpha_{t-1}} \right] \right\} \\
&= \exp \left\{ -\frac{1}{2} \left[ \frac{\alpha_t}{(1 - \alpha_t)} + \frac{1}{1 - \alpha_{t-1}} \right] x_{t-1}^2 - 2 \left( \frac{\sqrt{\alpha_t} x_t}{1 - \alpha_t} + \frac{\sqrt{\alpha_{t-1}} x_0}{1 - \alpha_{t-1}} \right) x_{t-1} \right\} \\
&= \exp \left\{ -\frac{1}{2} \left[ \frac{\alpha_t (1 - \alpha_{t-1}) + 1 - \alpha_t}{(1 - \alpha_t)(1 - \alpha_{t-1})} \right] x_{t-1}^2 - 2 \left( \frac{\sqrt{\alpha_t} x_t + \sqrt{\alpha_{t-1}} x_0}{1 - \alpha_t} \right) x_{t-1} \right\} \\
&= \exp \left\{ -\frac{1}{2} \left[ \frac{\alpha_t - \alpha_t \alpha_{t-1} + 1 - \alpha_t}{(1 - \alpha_t)(1 - \alpha_{t-1})} \right] x_{t-1}^2 - 2 \left( \frac{\sqrt{\alpha_t} x_t + \sqrt{\alpha_{t-1}} x_0}{1 - \alpha_t} \right) x_{t-1} \right\} \\
&= \exp \left\{ -\frac{1}{2} \left[ \frac{1 - \alpha_t}{(1 - \alpha_t)(1 - \alpha_{t-1})} \right] x_{t-1}^2 - 2 \left( \frac{\sqrt{\alpha_t} x_t + \sqrt{\alpha_{t-1}} x_0}{1 - \alpha_t} \right) x_{t-1} \right\} \\
&= \exp \left\{ -\frac{1}{2} \left[ \frac{1}{(1 - \alpha_t)(1 - \alpha_{t-1})} \right] x_{t-1}^2 - 2 \left( \frac{\sqrt{\alpha_t} x_t + \sqrt{\alpha_{t-1}} x_0}{(1 - \alpha_t)(1 - \alpha_{t-1})} \right) x_{t-1} \right\} \\
&= \exp \left\{ -\frac{1}{2} \left[ \frac{1}{(1 - \alpha_t)(1 - \alpha_{t-1})} \right] \left[ x_{t-1}^2 - 2 \frac{(\sqrt{\alpha_t} x_t + \sqrt{\alpha_{t-1}} x_0)(1 - \alpha_t)(1 - \alpha_{t-1})}{1 - \alpha_t} x_{t-1} \right] \right\} \\
&= \exp \left\{ -\frac{1}{2} \left[ \frac{1}{(1 - \alpha_t)(1 - \alpha_{t-1})} \right] \left[ x_{t-1}^2 - 2 \frac{\sqrt{\alpha_t}(1 - \alpha_{t-1}) x_t + \sqrt{\alpha_{t-1}}(1 - \alpha_t) x_0}{1 - \alpha_t} x_{t-1} \right] \right\} \\
&\propto \mathcal{N}(x_{t-1}; \frac{\sqrt{\alpha_t}(1 - \alpha_{t-1}) x_t + \sqrt{\alpha_{t-1}}(1 - \alpha_t) x_0}{1 - \alpha_t}, \frac{(1 - \alpha_t)(1 - \alpha_{t-1})}{2\alpha_t(t)})
\end{align*}</script><h3 id="lemma-7"><a href="#lemma-7" class="headerlink" title="lemma 7"></a>lemma 7</h3><script type="math/tex; mode=display">
\begin{align*}
&\arg\min_{\theta} D_{KL}(q(x_{t-1}|x_t, x_0) \parallel p_\theta(x_{t-1}|x_t)) \\
&= \arg\min_{\theta} D_{KL}(\mathcal{N}(x_{t-1}; \mu_q, \Sigma_q(t)) \parallel \mathcal{N}(x_{t-1}; \mu_\theta, \Sigma_q(t))) \\
&= \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \left[ \left\| \frac{\sqrt{\alpha_t}(1 - \alpha_{t-1})x_t + \sqrt{\alpha_{t-1}}(1 - \alpha_t)\hat{x}_\theta(x_t, t)}{1 - \bar{\alpha}_t} - \frac{\sqrt{\alpha_t}(1 - \alpha_{t-1})x_t + \sqrt{\alpha_{t-1}}(1 - \alpha_t)x_0}{1 - \bar{\alpha}_t} \right\|_2^2 \right] \\
&= \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \left[ \left\| \frac{\sqrt{\alpha_{t-1}}(1 - \alpha_t)\hat{x}_\theta(x_t, t)}{1 - \bar{\alpha}_t} - \frac{\sqrt{\alpha_{t-1}}(1 - \alpha_t)x_0}{1 - \bar{\alpha}_t} \right\|_2^2 \right] \\
&= \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \left[ \left\| \frac{\sqrt{\alpha_{t-1}}(1 - \alpha_t)}{1 - \bar{\alpha}_t} (\hat{x}_\theta(x_t, t) - x_0) \right\|_2^2 \right] \\
&= \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \frac{\alpha_{t-1}(1 - \alpha_t)^2}{(1 - \bar{\alpha}_t)^2} \left[ \left\| \hat{x}_\theta(x_t, t) - x_0 \right\|_2^2 \right] \\
&= \arg\min_{\theta} \frac{1}{2 \frac{(1 - \alpha_t)(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}} \frac{\bar{\alpha}_{t-1}(1 - \alpha_t)^2}{(1 - \bar{\alpha}_t)^2} \left[ \left\| \hat{x}_\theta(x_t, t) - x_0 \right\|_2^2 \right] \\
&= \arg\min_{\theta} \frac{1}{2} \frac{1 - \bar{\alpha}_t}{(1 - \alpha_t)(1 - \bar{\alpha}_{t-1})} \frac{\bar{\alpha}_{t-1}(1 - \alpha_t)^2}{(1 - \bar{\alpha}_t)^2} \left[ \left\| \hat{x}_\theta(x_t, t) - x_0 \right\|_2^2 \right] \\
&= \arg\min_{\theta} \frac{1}{2} \frac{\bar{\alpha}_{t-1}(1 - \alpha_t)}{(1 - \bar{\alpha}_{t-1})(1 - \bar{\alpha}_t)} \left[ \left\| \hat{x}_\theta(x_t, t) - x_0 \right\|_2^2 \right] \\
&= \arg\min_{\theta} \frac{1}{2} \frac{\bar{\alpha}_{t-1} - \bar{\alpha}_t}{(1 - \bar{\alpha}_{t-1})(1 - \bar{\alpha}_t)} \left[ \left\| \hat{x}_\theta(x_t, t) - x_0 \right\|_2^2 \right] \\
&= \arg\min_{\theta} \frac{1}{2} \frac{\bar{\alpha}_{t-1} - \bar{\alpha}_t - 1}{(1 - \bar{\alpha}_{t-1})(1 - \bar{\alpha}_t)} \left[ \left\| \hat{x}_\theta(x_t, t) - x_0 \right\|_2^2 \right] \\
&= \arg\min_{\theta} \frac{1}{2} \left( \frac{\bar{\alpha}_{t-1}(1 - \bar{\alpha}_t)}{1 - \bar{\alpha}_{t-1}(1 - \bar{\alpha}_t)} - \frac{\bar{\alpha}_t(1 - \bar{\alpha}_{t-1})}{(1 - \bar{\alpha}_{t-1})(1 - \bar{\alpha}_t)} \right) \left[ \left\| \hat{x}_\theta(x_t, t) - x_0 \right\|_2^2 \right] \\
&= \arg\min_{\theta} \frac{1}{2} \left( \frac{\bar{\alpha}_{t-1}}{1 - \bar{\alpha}_{t-1}} - \frac{\bar{\alpha}_t}{1 - \bar{\alpha}_t} \right) \left[ \left\| \hat{x}_\theta(x_t, t) - x_0 \right\|_2^2 \right]
\end{align*}</script><h3 id="lemma-8"><a href="#lemma-8" class="headerlink" title="lemma 8"></a>lemma 8</h3><script type="math/tex; mode=display">
\begin{align*}
&\arg\min_{\theta} D_{KL}(q(x_{t-1}|x_t, x_0) \parallel p_\theta(x_{t-1}|x_t)) \\
&= \arg\min_{\theta} D_{KL}(\mathcal{N}(x_{t-1}; \mu_q, \Sigma_q(t)) \parallel \mathcal{N}(x_{t-1}; \mu_\theta, \Sigma_q(t))) \\
&= \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \left[ \left\| \frac{1}{\sqrt{\alpha_t}} x_t - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}\sqrt{\alpha_t}} \hat{\epsilon}_\theta(x_t, t) - \frac{1}{\sqrt{\alpha_t}} x_t + \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}\sqrt{\alpha_t}} \epsilon_0 \right\|_2^2 \right] \\
&= \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \left[ \left\| \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}\sqrt{\alpha_t}} \epsilon_0 - \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}\sqrt{\alpha_t}} \hat{\epsilon}_\theta(x_t, t) \right\|_2^2 \right] \\
&= \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \left[ \left\| \frac{1 - \alpha_t}{\sqrt{1 - \bar{\alpha}_t}\sqrt{\alpha_t}} (\epsilon_0 - \hat{\epsilon}_\theta(x_t, t)) \right\|_2^2 \right] \\
&= \arg\min_{\theta} \frac{1}{2\sigma_q^2(t)} \frac{(1 - \alpha_t)^2}{(1 - \bar{\alpha}_t)\alpha_t} \left[ \left\| \epsilon_0 - \hat{\epsilon}_\theta(x_t, t) \right\|_2^2 \right]
\end{align*}</script>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Diffusion/" rel="tag"># Diffusion</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2025/02/24/hello-world/" rel="prev" title="Hello World">
      <i class="fa fa-chevron-left"></i> Hello World
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="gitalk-container"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.</span> <span class="nav-text">Introduction</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VAE"><span class="nav-number">2.</span> <span class="nav-text">VAE</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MHVAE-%EF%BC%88Markovian-Hierarchical-Variational-Autoencoders%EF%BC%89"><span class="nav-number">3.</span> <span class="nav-text">MHVAE （Markovian Hierarchical Variational Autoencoders）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#VDM-Variational-Diffusion-Model"><span class="nav-number">4.</span> <span class="nav-text">VDM (Variational Diffusion Model)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lemmas"><span class="nav-number">5.</span> <span class="nav-text">Lemmas</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#lemma1-%E6%8B%86%E5%88%86likelihood%E8%8E%B7%E5%BE%97ELBO"><span class="nav-number">5.1.</span> <span class="nav-text">lemma1: 拆分likelihood获得ELBO</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lemma2-ELBO%E5%88%86%E8%A7%A3"><span class="nav-number">5.2.</span> <span class="nav-text">lemma2: ELBO分解</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lemma3-VDM-ELBO%E6%8E%A8%E5%AF%BC1"><span class="nav-number">5.3.</span> <span class="nav-text">lemma3: VDM ELBO推导1</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lemma-5-p-x-t-x-0-%E5%BE%97%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B"><span class="nav-number">5.4.</span> <span class="nav-text">lemma 5: $p(x_t|x_0)$得推导过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lemma-6-p-x-t-1-x-t-x-0-%E6%8E%A8%E5%AF%BC%E8%BF%87%E7%A8%8B"><span class="nav-number">5.5.</span> <span class="nav-text">lemma 6: $p(x{t-1}|x{t},x_0)$推导过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lemma-7"><span class="nav-number">5.6.</span> <span class="nav-text">lemma 7</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#lemma-8"><span class="nav-number">5.7.</span> <span class="nav-text">lemma 8</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Youyin Wang"
      src="/images/avatar.png">
  <p class="site-author-name" itemprop="name">Youyin Wang</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/YouyinWang" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;YouyinWang" rel="noopener" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wangyaya1996@gmail.com" title="E-Mail → mailto:wangyaya1996@gmail.com" rel="noopener" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Youyin Wang</span>
</div>
  <div class="powered-by">
    <!--由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动 -->
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  
















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css">

<script>
NexT.utils.loadComments(document.querySelector('#gitalk-container'), () => {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js', () => {
    var gitalk = new Gitalk({
      clientID    : 'Ov23litmadIDsYoyENgu',
      clientSecret: 'ca83b6a79ef4eddb2d3dbb552d184d974b2c24c8',
      repo        : 'youyin.github.io',
      owner       : 'YouyinWang',
      admin       : ['YouyinWang'],
      id          : '8899fc907ebdfd831f58399bc9ff8c7b',
        language: 'zh-CN',
      distractionFreeMode: true
    });
    gitalk.render('gitalk-container');
  }, window.Gitalk);
});
</script>

</body>
</html>
